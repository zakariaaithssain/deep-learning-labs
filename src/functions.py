from sklearn.metrics import classification_report, ConfusionMatrixDisplay

from torch.utils.data import DataLoader
from torchmetrics.classification import BinaryRecall

import matplotlib.pyplot as plt 
import torch 
import logging 

from src.config import setup_logging, DOTPT_FILE_PATH
setup_logging()





def train(model, train_dataloader:DataLoader, num_epochs:int, lr:float, device:torch.device, valid_dataloader:DataLoader, patience: int=20): 
        logger = logging.getLogger("train")
        model.to(device)
        #Adam with L2 regularization, regularization strength 1e-4
        decay, no_decay = [], []
        for _, param in model.named_parameters():
            if param.ndim == 1:  #bias
                no_decay.append(param)
            else: #weights
                decay.append(param)

        optimizer = torch.optim.AdamW(
            [
                {"params": decay, "weight_decay": 1e-4},
                {"params": no_decay, "weight_decay": 0.0},
            ],
            lr=lr
        )

        criterion = torch.nn.CrossEntropyLoss()
        best_val_loss = float("inf")
        patience_counter = 0

        logger.info("training started.")
        for epoch in range(num_epochs): 
                model.train()
                train_loss = 0.0
                n_batches = 0
                for X_batch, y_batch in train_dataloader: 
                        # to calculate the mean of loss per batch (train_loss/n_batches)
                        n_batches+=1
                        #tensors should be at the same device as the model
                        X_batch = X_batch.to(device)
                        y_batch = y_batch.to(device)
                        #forward pass
                        logits = model(X_batch)
                        batch_loss = criterion(logits, y_batch)

                        train_loss+= batch_loss.item()

                        #backward pass
                        optimizer.zero_grad()
                        batch_loss.backward()

                        #update params 
                        optimizer.step()
                train_loss/= n_batches
                
                # validation for early stopping
                val_loss = validate(model, criterion, valid_dataloader, device)

                logger.debug(
                    f"epoch {epoch:03d} | train cross entropy {train_loss:.4f} | val cross entropy {val_loss:.4f}"
                )

                # early stopping
                if val_loss <= best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    logger.debug(f"patience counter reinitialized.")
                    #save the state of the best model so far, allows as to increase patience, that will prevent stopping because of noise. 
                    torch.save(
                        {"best_model_state":model.state_dict(),
                        "best_validation_loss": best_val_loss,
                        "epoch":epoch}, 
                                DOTPT_FILE_PATH)
                else:
                    patience_counter += 1
                    logger.debug(f"validation cross entropy did not improve at epoch {epoch}")

                if patience_counter >= patience:
                    logger.warning(f"early stopping triggered after {patience} waiting epochs. min cross entropy loss: {best_val_loss}; epoch: {epoch}; best state saved to 'best_state.pt'")
                    break

                if epoch % 10 == 0 or epoch==99: 
                    logger.info(f"epoch {epoch:03d} | train cross entropy {train_loss:.4f} | val cross entropy {val_loss:.4f}")
         
        logger.info("training completed. ")



#do not track gradient
@torch.no_grad()
def validate(model, criterion, valid_dataloader:DataLoader, device:torch.DeviceObjType):
    "to be used for `early stopping` inside `train`"
    model.eval()
    total_loss = 0.0
    num_batches = 0
    for X_batch, y_batch in valid_dataloader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)

        logits = model(X_batch)
        loss = criterion(logits, y_batch)

        total_loss += loss.item()
        num_batches += 1

    return total_loss / num_batches

           
                  


@torch.no_grad()
def test(model, test_data:torch.utils.data.Dataset, device:torch.DeviceObjType, metric = BinaryRecall()):
    "testing using the `Binary Recall` metric by default, as we care more about minimizing false negatives (`FN`)"
    model.to(device)
    model.eval()
    dataloader = torch.utils.data.DataLoader(test_data, batch_size=37, shuffle=True,
                                            pin_memory=True) #in case we have GPU
    
    logger = logging.getLogger("test")
    logger.info("final testing started.")
    num_batches = 0
    for X_batch, y_batch in dataloader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        logits = model(X_batch) #this gives a logit for each class cuz we used cross entropy
        preds = torch.argmax(logits, dim=1) 
        batch_recall = metric(preds, y_batch) 
        logger.info(f"Recall for batch {num_batches}: {batch_recall.item()}")
        num_batches += 1

    #get the aggregated result across all batches
    total_recall = metric.compute()
    logger.info(f"Recall on all testing data: {total_recall}")

    metric.reset()



def report_and_conf_matrix(model, X_test:torch.Tensor, y_test:torch.Tensor):
    """build scikit learn classfication report and confusion matrix from the testing data.  
    I didn't use separated functions to avoid redundant model calls"""

    with torch.no_grad():
        logits = model(X_test)
        preds = torch.argmax(logits, dim=1) 

    #return tensors to cpu and convert them to ndarrays (sklearn doesn't support tensors)
    y_true = y_test.cpu().numpy()
    y_pred = preds.cpu().numpy()

    report = classification_report(
        y_true,
        y_pred,
        target_names=["healthy", "sick"]
    )
    print(report)
    #confusion matrix
    ConfusionMatrixDisplay.from_predictions(y_true,
                                            y_pred,
                                            display_labels=["healthy", "sick"],
                                            cmap="Blues",
                                            normalize='true'
                                               )
    plt.title("confusion matrix")
    plt.tight_layout()
    plt.savefig("confusion_matrix.png", dpi=300, bbox_inches="tight")
    plt.show()
     
